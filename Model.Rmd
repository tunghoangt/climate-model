---
title: "Project_3_SST"
output: html_document
author:
- Tony Hoang
- Carlos Martinez
- Md Ayub Ali Sarker
- Thomas Duffy
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ncdf4)
library(glmnet)
library(MASS)
library(biglm)
library(gtools)
library(leaps)
```

### Data Input

First, we must begin by loading in the data from the netCDF file format containing the measurements over each month. 
```{r init_load}
# load-in months from data folder (user-specific)
locations  <- c("../data/matchups_NPP_2018-01.nc", "../data/matchups_NPP_2018-02.nc",
                "../data/matchups_NPP_2018-03.nc","../data/matchups_NPP_2018-04.nc")

# pull in the variables from the files and bind together
bright_3.7 <- vector("numeric")
bright_4   <- vector("numeric")
bright_8.6 <- vector("numeric")
bright_11  <- vector("numeric")
bright_12  <- vector("numeric")
sec_theta  <- vector("numeric")
sst_ref    <- vector("numeric")
sst_reg    <- vector("numeric")
sst_act    <- vector("numeric")
sza        <- vector("numeric")
for( loc in locations ) {
  open_file  <- nc_open(loc)
  sza        <- c(sza, ncvar_get(open_file, 'sza'))
  bright_3.7 <- c(bright_3.7, ncvar_get(open_file, 'BT_M12'))
  bright_4   <- c(bright_4, ncvar_get(open_file, 'BT_M13'))
  bright_8.6 <- c(bright_8.6, ncvar_get(open_file, 'BT_M14'))
  bright_11  <- c(bright_11, ncvar_get(open_file, 'BT_M15'))
  bright_12  <- c(bright_12, ncvar_get(open_file, 'BT_M16'))
  sec_theta  <- c(sec_theta, ncvar_get(open_file, 'vza'))
  sst_ref    <- c(sst_ref, ncvar_get(open_file, 'sst_ref'))
  sst_reg    <- c(sst_reg, ncvar_get(open_file, 'sst_reg'))
  sst_act    <- c(sst_act, ncvar_get(open_file, 'sst_insitu'))
}

# create data frame
df    <- data.frame(bright_3.7 = bright_3.7, bright_4 = bright_4, bright_8.6 = bright_8.6, bright_11 = bright_11,
                    bright_12 = bright_12, sec_theta = sec_theta, sst_ref = sst_ref, sst_reg = sst_reg,
                    sst_act = sst_act, sza = sza)
# indicator of whether it's day time
df$is_day   <- as.integer(df$sza < 95)

# clean out memory by removing all the vectors from this step
rm(open_file, bright_3.7, bright_4, bright_8.6, bright_11, bright_12, sec_theta, sst_ref, sst_reg, sst_act, sza)
```

```{r }
# what does df look like?
head(df)
```

### Model Introduction

Anding and Kauth (1970) found that the difference in measurement at properly selected infrared (IR) channels is proportional to the required amount of atmospheric correction.

Barton (1995) used this differential absorption between the channels which is used in all IR sea surface temperature (SST) model. In its basic form it usually represented as:

$$
T_S = aT_{\lambda_i}+\gamma(T_{\lambda_i}-T_{\lambda_j})+c
$$

where $T_s$ is estimated SST, $T_{\lambda_i}$ and $T_{\lambda_j}$ are brigthness temperature measurements in channels $\lambda_i$ and $\lambda_j$ where $i\ne j$. Therefore, the trick is to estimated which channels must be used. Both, $a$ and $c$ are constants. Finally, $\gamma$ is defined as

$$
\gamma=\frac{1-\tau_{\lambda_i}}{\tau_{\lambda_i}-\tau_{\lambda_j}}
$$

where $\tau$ is the transmittance through the atmosphere from the sea surface to the signal receiving satellite.

Though all statistical models share the above form, various modifications have been made over time to improve performance. One such model is based on the non-linear SST algorithm (NLSST) which was originally developed by Walton et al. (1998) which has the form:

$$
T_S=b_0+b_1T_{\lambda_i}+b_2(T_{\lambda_i}-T_{\lambda_j})T_{ref}+b_3(T_{\lambda_i}-T_{\lambda_j})(\sec\theta-1)
$$

where $T_{ref}$ is a best-first-guess SST value and $\sec\theta$ is the satellite zenith angle. 

For the below analysis, given a limited amount of resources available, we must pull out random subsets from our data in order to fit the models. Below, we'll construct a mask of training & testing data (they have no intersection) of approximately 10% of the original data. Given that ```R``` stores its objects in RAM, it's not possible to do fittings much larger.

```{r}
set.seed(42)
TRAIN_SIZE <- .1
train_mask <- sample.int(n = nrow(df), size = floor(nrow(df) * TRAIN_SIZE), replace = F)
TEST_SIZE  <- .1
test_mask  <- setdiff(sample.int(n = nrow(df), size = floor(nrow(df) * TEST_SIZE), replace = F), train_mask)

# how large is our test set?
length(test_mask)/nrow(df)
```

### Part 1

```{r}
x    <- c('bright_3.7','bright_4','bright_8.6','bright_11','bright_12')
perm <- gtools::permutations(n = 5, r = 2, v = x)

# Generating all possible linear formula with pair of I and J (20 formula total)
formula <- list(length = nrow(perm))
for (index in 1:nrow(perm)) {
  formula[[index]] <- as.formula(paste("sst_act~",toString(perm[index,1]),
                     "+I(",toString(perm[index,1]),"-",toString(perm[index,2]),")",sep=""))
}
head(formula)
```

```{r}
nfolds      <- 5
fold.labels <- sample(rep(1:nfolds, length = floor(nrow(df)) * TEST_SIZE), replace = F)

# Setting up MSE matrix with formula as column names and n-fold as row label
mse.models           <- matrix(NA, nrow = nfolds, ncol = nrow(perm))
colnames(mse.models) <- as.character(formula)

# Running CV:
for (fold in 1:nfolds) {
  test.rows       <- setdiff(which(fold.labels == fold), train_mask)
  for (form in formula) {
    current.model <- lm(formula = form, data = df[test.rows,])
    predictions   <- predict(current.model, newdata = df[test_mask,])
    mse.models[fold, as.character(c(form))] <- mean((df[test_mask,]$sst_act - predictions)^2)
  }
}
rm(current.model, predictions)

# CV results:
cv.results <- as.matrix(colMeans(mse.models))
cv.results
```

```{r}
plot(1:20, cv.results, xlab = "models", ylab = "MSE", main = "MSE results for CV with 5 fold", type = "b")
```

### Part 2

Walton et al. (1988) proposed the following model to estimate sst:

$$
T_S=b_0+b_1T_{\lambda_i}+b_2(T_{\lambda_i}-T_{\lambda_j})T_{ref}+b_3(T_{\lambda_i}-T_{\lambda_j})(\sec\theta-1)
$$

Here, we must find the best channels $T_{\lambda_i}$ and $T_{\lambda_j}$ which is possible to do by exhaustive search just the small search space. Then, we can pick the best model by investigating the mean squared error and performance on the outstanding test set.

Setting up all possible pairs of I and J for the model. There are 20 possible combinations
```{r}
x <- c('bright_3.7','bright_4','bright_8.6','bright_11','bright_12')
perm <- gtools::permutations( n = 5, r = 2, v = x)

#Generating all possible linear formula with pair of I and J (20 formula total)
formula <- list( length = nrow(perm))
for (index in 1:nrow(perm)) {
  formula[[index]] <- as.formula(c(paste("sst_act~",toString(perm[index,1]),
                     "+I(",toString(perm[index,1]),"-",toString(perm[index,2]),"):sec_theta",sep="")))
}
head(formula)
```

Setting up n-fold Cross-Validation:
```{r}
nfolds <- 5
fold.labels <- sample(rep(1:nfolds, length = floor(nrow(df)) * TEST_SIZE), replace = F)

#Setting up MSE matrix with formula as column names and n-fold as row label
mse.models <- matrix(NA, nrow = nfolds, ncol = nrow(perm))
colnames(mse.models) <- as.character(formula)

#Running CV:
for (fold in 1:nfolds) {
  test.rows <- setdiff(which(fold.labels==fold), train_mask)
  for (form in formula) {
    current.model <- lm(formula = form, data = df[test.rows,])
    predictions <- predict(current.model, newdata = df[test_mask,])
    mse.models[fold, as.character(c(form))] <- mean((df[test_mask,]$sst_act - predictions)^2)
  }
}
rm(current.model, predictions)

#CV results:
cv.results <- as.matrix(colMeans(mse.models))
cv.results
```

Plotting the result for model comparison:
```{r}
plot(1:20, cv.results, xlab = "models", ylab = "MSE", main = "MSE results for CV with 5 fold", type = "b")
```

The model that has the lowest MSE is with I = 11 and J = 3.7
```{r}
names(cv.results[which.min(cv.results),])
mse.model <- cv.results[which.min(cv.results)]
mse.model
```

Checking the performance of the operational model, measured by MSE:
```{r}
mse.Top = mean((df$sst_reg - df$sst_act)^2)
mse.Top 
```

The MSE of the operational model is much lower than the selected model. Therefore, the selected model is not performing as well.

### Part 3

To perform a subset selection of the predicitors for the model below, 

$$
T_S=b_0+b_1T_{\lambda_i}+b_2(T_{\lambda_i}-T_{\lambda_j})T_{ref}+b_3(T_{\lambda_i}-T_{\lambda_j})(\sec\theta-1)+b_4T_{ref}
$$

we will use the best subset algorithm, which gives the total of possible models that can be created from $p$ variables selecting $k$ number of predictors.

$$
{p \choose k}=\frac{p!}{k!(p-k)!}
$$

#### Data Preparation

Due to our model consist of three predictors and two of the predictors is the difference between two variables. To calculate the total number of variables in our data, it is necessary to violate the Best subset selection algorithm. This algorithm is limited to analyze sets of pairs. It does not hold for pair repetition. In our model each variable is used twiced if and only if both the variables have different wavelengths. In other words, the model establishes a different order each time a variable is used. To assert the total of possible models, mathematically this can be expressed as:

$$
k!{p \choose k} = \frac{p!}{(p-k)!}
$$

To account for the order of each variable used, it is convenient to estimate the amount of possible by permutation instead of combinations. In total we will have 20 possible models, from which the model with the highest adjusted $R^2_{adj}$ will be selected. 

```{r Data Preparation}
# permutation of all possible variables combination for the model
x     <- c('bright_3.7', 'bright_4', 'bright_8.6', 'bright_11', 'bright_12')
p     <- 5
k     <- 2
permu <- data.frame(gtools::permutations(n = p, r = k, v = x))
```

```{r}
# Dimensions of the sample
print(dim(df[train_mask,]))
# Dimensions of the permutation
print(dim(permu))
# Permutation data frame
print(permu)
```

#### Model Selection
To select the best model, the goal is to identify the model with maximum adjusted $R^{2}$. We developed an algorithm to extract $R^{2}$ from each possible model.

```{r Model Selection}
# Storage lists set up
adj_rs <- vector("numeric")
os_err <- vector("numeric")

# loop through permutations of model
for (index in 1:nrow(permu)) {
  formula <- as.formula(paste("sst_act ~", toString(permu[index, 1]), "+I(",toString(permu[index,1]), "-",                                         toString(permu[index, 2]), ")+ I(", toString(permu[index,1]), "-",
                              toString(permu[index, 2]), "):sec_theta", "+ sst_ref", sep=""))
  model               <- lm(formula = formula, data = df[train_mask,])
  prediction          <- predict(model, df[test_mask,], type = "response")
  adj_rs[[index]]     <- summary(model)$adj.r.squared
  os_err[[index]]     <- mean( (df[test_mask,]$sst_act - prediction)^2 )
}
rm(model, prediction) # memory management

dff <- data.frame(permutations = permu, adjusted_R2 = adj_rs, OS_error = os_err)
mse.Top; dff
```

Based on the results, it can be concluded all models provide the same accuracy of prediction. Note that the adjusted $R^{2}$ for each of our model is extremetly close to one.

### Part 4
Adding further variables and segmenting the model based on whether it's day or night using the solar zenith angel ```sza``` to threshold, we can enrich our model which then requires the estimation of 14 parameters.

In the daytime the model has the form:
$$
T_S=b_0+b_1T_{11}+b_2(T_{11}-T_{3.7}) + b_3(T_{11}-T_{8.6})+b_4(T_{11}-T_{12})\\
+(\sec\theta-1)[b_5+b_6T_{11}+b_7(T_{11}-T_{3.7})+b_8(T_{11}-T_{8.6})+b_9(T_{11}-T_{12})]\\
+T_{ref}[b_{10}T_{11}+b_{11}(T_{11}-T_{3.7})+b_{12}(T_{11}-T_{8.6})+b_{13}(T_{11}-T_{12})]
$$

And, in the evening: 
$$
T_S=b_0+b_1T_{11}+b_2(T_{11}-T_{8.6}) +b_3(T_{11}-T_{12})\\
+(\sec\theta-1)[b_4+b_5T_{11}+b_6(T_{11}-T_{8.6})+b_7(T_{11}-T_{12})]\\
+T_{ref}[b_8T_{11}+b_9(T_{11}-T_{8.6})+b_{10}(T_{11}-T_{12})]
$$

We'll need to make some modifications to our original data to include these differences.

```{r df_mod}
df$diff_11_3.7 <- df$is_day * (df$bright_11 - df$bright_3.7)  # include if daytime indicator
df$diff_11_8.6 <- df$bright_11 - df$bright_8.6
df$diff_11_12  <- df$bright_11 - df$bright_12
head(df)
```

Notice, the difference between the nighttime and daytime model is the exclusion of the $T_{11}-T_{3.7}$ channel in the evening, so using our indicator variable ```is_day``` we can simplify our implementation.

Now, we must estimate our coefficient vector $\vec{b}$ for each of the models defined above. Here we will use the shrinkage methods of LASSO and Ridge regression. 

#### Model Fitting

Generally, we know that in fitting a model with least squares we need to find the estimates $\vec{b}$ such that

$$
\vec{b}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2} 
$$
for $i$ observations and $p$ parameters. 

In penalized regression (of which both LASSO and Ridge are specific cases), we want to avoid overfitting, or adding too many $x_j$ to our model if they do not provide a significant benefit. This way we maintain a rich model while not over complicating it unless necessary. In Ridge, then we tackle the following minimization problem:
$$
\vec{b}_{Ridge}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2}+\lambda\sum_{j=1}^pb^2_j 
$$

where now we have an additional penalty term for adding a parameter to the model.

In LASSO, the minimization problem takes the form:
$$
\vec{b}_{LASSO}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2}+\lambda\sum_{j=1}^p|b_j| 
$$

Here, the goal is to predict ```sst_insitu``` with maximal accuracy as this is the actual "in the water" temperature that we are attempting to predict. We can then compare the performance of our model with the operational ```sst_reg```.

Within the ```glmnet()``` function of the ```glmnet``` library, the parameter ```alpha``` specifies the elasticnet mixing parameter. The penalty within the function is defined:
$$
\frac{(1-\alpha)}{2}\lVert\beta\rVert^2_2+\alpha\lVert\beta\rVert_1
$$

So, ```alpha=1``` simplifies to the LASSO penalty and ```alpha=0``` simplifies to the Ridge penalty. The ```glmnet()``` function requires our x variables represented as a matrix, so we'll need to perform some pre-processing to ensure it runs smoothly. We'll use different train and tests subsets of our data to get a better sense of out-of-sample performance for our models. 

```{r model_prep}
# split into train & test sets for more robust performance evaluation
form        <- formula(sst_act ~ bright_11 + diff_11_3.7 + diff_11_8.6 + diff_11_12 + 
                     bright_11:sec_theta + diff_11_3.7:sec_theta + diff_11_8.6:sec_theta + diff_11_12:sec_theta +
                     bright_11:sst_ref + diff_11_3.7:sst_ref + diff_11_8.6:sst_ref + diff_11_12:sst_ref)
X_train     <- model.matrix(form, df[train_mask,])
y_train     <- as.matrix(df$sst_act[train_mask], ncol = 1)
```

Now, we can fit both our LASSO and Ridge models on our training set and then make predictions on our excluded test set.

```{r fit}
fit_lasso <- glmnet::glmnet(X_train, y_train, alpha = 1)
fit_ridge <- glmnet::glmnet(X_train, y_train, alpha = 0)
rm(X_train, y_train) # memory management
```

```{r pred}
X_test         <- model.matrix(form, df[test_mask,])
y_test         <- as.matrix(df$sst_act[test_mask], ncol = 1)

# predict 
predict_df         <- data.frame(y_actual = y_test)
predict_df$y_lasso <- predict(fit_lasso, X_test, s = min(fit_lasso$lambda), type = "response")
predict_df$y_ridge <- predict(fit_ridge, X_test, s = min(fit_ridge$lambda), type = "response")
rm(X_test, y_test) # memory management
```

What about their prediction accuracy on the test set? How is it compared to the error from the operational model? Let's use the root-mean squared error over our test set as the metric, which has the form:

$$
RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2}
$$

where $y$ is the actual value and $\hat{y}$ is the corresponding model predicted value.

```{r error}
rmse <- function(y_act, y_pred) {
  return( sqrt( length(y_pred)^-1 * sum((y_pred - y_act)^2) ))
}

# add normalized function
nrmse <- function(y_act, y_pred) {
  return( rmse(y_act, y_pred) / mean(y_act) )
}

error_list <- list(
  base_rmse = rmse(df$sst_act[test_mask], df$sst_reg[test_mask]),
  lasso_rmse = rmse(df$sst_act[test_mask], predict_df$y_lasso),
  ridge_rmse = rmse(df$sst_act[test_mask], predict_df$y_ridge),
  base_nrmse = nrmse(df$sst_act[test_mask], df$sst_reg[test_mask]), 
  lasso_nrmse = nrmse(df$sst_act[test_mask], predict_df$y_lasso),
  ridge_rmse = nrmse(df$sst_act[test_mask], predict_df$y_ridge)
)
error_list
```

What do the solution pathways for both versions look like?

```{r model_vis}
plot(fit_lasso)
plot(fit_ridge)
```

What about across the lambda parameters?

```{r lambda}
plot(fit_lasso, xvar = "lambda")
plot(fit_ridge, xvar = "lambda")
```

In order to ensure there is not some interaction present that might threaten the robustness of our model, we can investigate the distribution of our residuals, $\hat{y_i}-y_i$ for both the models. 

```{r hist}
hist(predict_df$y_ridge - predict_df$y_actual, main = "Ridge Regression Residuals", xlab = "Distance from Actual",
     col = 3)
hist(predict_df$y_lasso - predict_df$y_actual, main = "LASSO Regression Residuals", xlab = "Distance from Actual",
     col = 5)
```

In both cases, they appear to be relatively normal, which should provide us with further confidence that our model is justifiable. 

### Part 5 
Returning to our model that segments depending on time of day from Part 4, perhaps the usage of forward or backward selection would perform just as well, or better. The forward selection process begins with a null and then iteratively adds a variable that maximizes the criteria in a step-wise fashion (the backward selection beginning with the full model and going in reverse). The two criterion used here will be the Akaike Information Criterion (AIC) and the Bayesian Information Criteria (BIC).

In the case of AIC we are fitting to minimize the following:

$$
AIC = 2p-2\ln(\hat{L})
$$

where $p$ is the number of parameters within our model and $\hat{L}$ is the maximum of the likelihood for the candidate model. The BIC has a similar form, but different penalty. It is defined:

$$
BIC = p\ln(n)-2\ln(\hat{L})
$$

where $n$ is the number of observations.

In order to perform the following procedures in an environment with limited storage resources, we'll have to subset the original data significantly given that ```R``` tends to store all of its objects in memory. Even only using 0.05% of our data will still leave us with 181,171 observations, which should be more than enough for our purposes.

Let's fit our models, by first fitting the full model (which is required by ```step()```:

```{r full_fit}
full_lm     <- lm(form, data = df[train_mask,])
summary(full_lm)
```

```{r stepwise}
fit_forward  <- MASS::stepAIC(full_lm, direction = "forward")
fit_backward <- MASS::stepAIC(full_lm, direction = "backward")
```

```{r anova}
fit_forward$anova; fit_backward$anova
```

And we can now compare them to our performance using the LASSO and Ridge results from Part 4.

```{r stepwise_performance}
# predict step-wise on test set
y_forward  <- predict(fit_forward, df[test_mask,], type = "response")
y_backward <- predict(fit_backward, df[test_mask,], type = "response")

list(
  forward_rmse = rmse(df[test_mask,]$sst_act, y_forward),
  backward_rmse = rmse(df[test_mask,]$sst_act, y_backward),
  forward_nrmse = nrmse(df[test_mask,]$sst_act, y_forward),
  backward_rmse = nrmse(df[test_mask,]$sst_act, y_backward)
)
```

So, this procedure generates a model that is more accurate than those determined by the LASSO & Ridge procedures (though again fit on a much smaller subset of data). We did not test the error on the entire dataset, which may explain the robust performance. That said, it appears to approximate the error of the current model, perhaps performing slightly better. It should be noted using either forward or backward selection does not remove any of the variables from the fully enriched model form we began with.